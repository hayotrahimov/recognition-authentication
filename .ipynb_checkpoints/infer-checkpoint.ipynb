{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cells': [{'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['# Face detection and recognition inference pipeline\\n',\n",
       "    '\\n',\n",
       "    'The following example illustrates how to use the `facenet_pytorch` python package to perform face detection and recogition on an image dataset using an Inception Resnet V1 pretrained on the VGGFace2 dataset.\\n',\n",
       "    '\\n',\n",
       "    'The following Pytorch methods are included:\\n',\n",
       "    '* Datasets\\n',\n",
       "    '* Dataloaders\\n',\n",
       "    '* GPU/CPU processing']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 1,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['from facenet_pytorch import MTCNN, InceptionResnetV1\\n',\n",
       "    'import torch\\n',\n",
       "    'from torch.utils.data import DataLoader\\n',\n",
       "    'from torchvision import datasets\\n',\n",
       "    'import numpy as np\\n',\n",
       "    'import pandas as pd\\n',\n",
       "    'import os\\n',\n",
       "    '\\n',\n",
       "    \"workers = 0 if os.name == 'nt' else 4\"]},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['#### Determine if an nvidia GPU is available']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 2,\n",
       "   'metadata': {},\n",
       "   'outputs': [{'name': 'stdout',\n",
       "     'output_type': 'stream',\n",
       "     'text': ['Running on device: cuda:0\\n']}],\n",
       "   'source': [\"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\\n\",\n",
       "    \"print('Running on device: {}'.format(device))\"]},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['#### Define MTCNN module\\n',\n",
       "    '\\n',\n",
       "    'Default params shown for illustration, but not needed. Note that, since MTCNN is a collection of neural nets and other code, the device must be passed in the following way to enable copying of objects when needed internally.\\n',\n",
       "    '\\n',\n",
       "    'See `help(MTCNN)` for more details.']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 3,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['mtcnn = MTCNN(\\n',\n",
       "    '    image_size=160, margin=0, min_face_size=20,\\n',\n",
       "    '    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\\n',\n",
       "    '    device=device\\n',\n",
       "    ')']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['#### Define Inception Resnet V1 module\\n',\n",
       "    '\\n',\n",
       "    'Set classify=True for pretrained classifier. For this example, we will use the model to output embeddings/CNN features. Note that for inference, it is important to set the model to `eval` mode.\\n',\n",
       "    '\\n',\n",
       "    'See `help(InceptionResnetV1)` for more details.']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 4,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': [\"resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\"]},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['#### Define a dataset and data loader\\n',\n",
       "    '\\n',\n",
       "    'We add the `idx_to_class` attribute to the dataset to enable easy recoding of label indices to identity names later one.']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 5,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['def collate_fn(x):\\n',\n",
       "    '    return x[0]\\n',\n",
       "    '\\n',\n",
       "    \"dataset = datasets.ImageFolder('../data/test_images')\\n\",\n",
       "    'dataset.idx_to_class = {i:c for c, i in dataset.class_to_idx.items()}\\n',\n",
       "    'loader = DataLoader(dataset, collate_fn=collate_fn, num_workers=workers)']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['#### Perfom MTCNN facial detection\\n',\n",
       "    '\\n',\n",
       "    'Iterate through the DataLoader object and detect faces and associated detection probabilities for each. The `MTCNN` forward method returns images cropped to the detected face, if a face was detected. By default only a single detected face is returned - to have `MTCNN` return all detected faces, set `keep_all=True` when creating the MTCNN object above.\\n',\n",
       "    '\\n',\n",
       "    'To obtain bounding boxes rather than cropped face images, you can instead call the lower-level `mtcnn.detect()` function. See `help(mtcnn.detect)` for details.']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 6,\n",
       "   'metadata': {},\n",
       "   'outputs': [{'name': 'stdout',\n",
       "     'output_type': 'stream',\n",
       "     'text': ['Face detected with probability: 0.999957\\n',\n",
       "      'Face detected with probability: 0.999927\\n',\n",
       "      'Face detected with probability: 0.999662\\n',\n",
       "      'Face detected with probability: 0.999873\\n',\n",
       "      'Face detected with probability: 0.999991\\n']}],\n",
       "   'source': ['aligned = []\\n',\n",
       "    'names = []\\n',\n",
       "    'for x, y in loader:\\n',\n",
       "    '    x_aligned, prob = mtcnn(x, return_prob=True)\\n',\n",
       "    '    if x_aligned is not None:\\n',\n",
       "    \"        print('Face detected with probability: {:8f}'.format(prob))\\n\",\n",
       "    '        aligned.append(x_aligned)\\n',\n",
       "    '        names.append(dataset.idx_to_class[y])']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['#### Calculate image embeddings\\n',\n",
       "    '\\n',\n",
       "    'MTCNN will return images of faces all the same size, enabling easy batch processing with the Resnet recognition module. Here, since we only have a few images, we build a single batch and perform inference on it. \\n',\n",
       "    '\\n',\n",
       "    'For real datasets, code should be modified to control batch sizes being passed to the Resnet, particularly if being processed on a GPU. For repeated testing, it is best to separate face detection (using MTCNN) from embedding or classification (using InceptionResnetV1), as calculation of cropped faces or bounding boxes can then be performed a single time and detected faces saved for future use.']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 7,\n",
       "   'metadata': {},\n",
       "   'outputs': [],\n",
       "   'source': ['aligned = torch.stack(aligned).to(device)\\n',\n",
       "    'embeddings = resnet(aligned).detach().cpu()']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['#### Print distance matrix for classes']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': 8,\n",
       "   'metadata': {},\n",
       "   'outputs': [{'name': 'stdout',\n",
       "     'output_type': 'stream',\n",
       "     'text': ['                angelina_jolie  bradley_cooper  kate_siegel  paul_rudd  \\\\\\n',\n",
       "      'angelina_jolie        0.000000        1.344806     0.781201   1.425579   \\n',\n",
       "      'bradley_cooper        1.344806        0.000000     1.256238   0.922126   \\n',\n",
       "      'kate_siegel           0.781201        1.256238     0.000000   1.366423   \\n',\n",
       "      'paul_rudd             1.425579        0.922126     1.366423   0.000000   \\n',\n",
       "      'shea_whigham          1.448495        0.891145     1.416447   0.985438   \\n',\n",
       "      '\\n',\n",
       "      '                shea_whigham  \\n',\n",
       "      'angelina_jolie      1.448495  \\n',\n",
       "      'bradley_cooper      0.891145  \\n',\n",
       "      'kate_siegel         1.416447  \\n',\n",
       "      'paul_rudd           0.985438  \\n',\n",
       "      'shea_whigham        0.000000  \\n']}],\n",
       "   'source': ['dists = [[(e1 - e2).norm().item() for e2 in embeddings] for e1 in embeddings]\\n',\n",
       "    'print(pd.DataFrame(dists, columns=names, index=names))']}],\n",
       " 'metadata': {'kernelspec': {'display_name': 'Python 3',\n",
       "   'language': 'python',\n",
       "   'name': 'python3'},\n",
       "  'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
       "   'file_extension': '.py',\n",
       "   'mimetype': 'text/x-python',\n",
       "   'name': 'python',\n",
       "   'nbconvert_exporter': 'python',\n",
       "   'pygments_lexer': 'ipython3',\n",
       "   'version': '3.7.3'}},\n",
       " 'nbformat': 4,\n",
       " 'nbformat_minor': 2}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Face detection and recognition inference pipeline\\n\",\n",
    "    \"\\n\",\n",
    "    \"The following example illustrates how to use the `facenet_pytorch` python package to perform face detection and recogition on an image dataset using an Inception Resnet V1 pretrained on the VGGFace2 dataset.\\n\",\n",
    "    \"\\n\",\n",
    "    \"The following Pytorch methods are included:\\n\",\n",
    "    \"* Datasets\\n\",\n",
    "    \"* Dataloaders\\n\",\n",
    "    \"* GPU/CPU processing\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 1,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from facenet_pytorch import MTCNN, InceptionResnetV1\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"from torch.utils.data import DataLoader\\n\",\n",
    "    \"from torchvision import datasets\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"\\n\",\n",
    "    \"workers = 0 if os.name == 'nt' else 4\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### Determine if an nvidia GPU is available\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 2,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Running on device: cuda:0\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\\n\",\n",
    "    \"print('Running on device: {}'.format(device))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### Define MTCNN module\\n\",\n",
    "    \"\\n\",\n",
    "    \"Default params shown for illustration, but not needed. Note that, since MTCNN is a collection of neural nets and other code, the device must be passed in the following way to enable copying of objects when needed internally.\\n\",\n",
    "    \"\\n\",\n",
    "    \"See `help(MTCNN)` for more details.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 3,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"mtcnn = MTCNN(\\n\",\n",
    "    \"    image_size=160, margin=0, min_face_size=20,\\n\",\n",
    "    \"    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\\n\",\n",
    "    \"    device=device\\n\",\n",
    "    \")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### Define Inception Resnet V1 module\\n\",\n",
    "    \"\\n\",\n",
    "    \"Set classify=True for pretrained classifier. For this example, we will use the model to output embeddings/CNN features. Note that for inference, it is important to set the model to `eval` mode.\\n\",\n",
    "    \"\\n\",\n",
    "    \"See `help(InceptionResnetV1)` for more details.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 4,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### Define a dataset and data loader\\n\",\n",
    "    \"\\n\",\n",
    "    \"We add the `idx_to_class` attribute to the dataset to enable easy recoding of label indices to identity names later one.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 5,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def collate_fn(x):\\n\",\n",
    "    \"    return x[0]\\n\",\n",
    "    \"\\n\",\n",
    "    \"dataset = datasets.ImageFolder('../data/test_images')\\n\",\n",
    "    \"dataset.idx_to_class = {i:c for c, i in dataset.class_to_idx.items()}\\n\",\n",
    "    \"loader = DataLoader(dataset, collate_fn=collate_fn, num_workers=workers)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### Perfom MTCNN facial detection\\n\",\n",
    "    \"\\n\",\n",
    "    \"Iterate through the DataLoader object and detect faces and associated detection probabilities for each. The `MTCNN` forward method returns images cropped to the detected face, if a face was detected. By default only a single detected face is returned - to have `MTCNN` return all detected faces, set `keep_all=True` when creating the MTCNN object above.\\n\",\n",
    "    \"\\n\",\n",
    "    \"To obtain bounding boxes rather than cropped face images, you can instead call the lower-level `mtcnn.detect()` function. See `help(mtcnn.detect)` for details.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 6,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Face detected with probability: 0.999957\\n\",\n",
    "      \"Face detected with probability: 0.999927\\n\",\n",
    "      \"Face detected with probability: 0.999662\\n\",\n",
    "      \"Face detected with probability: 0.999873\\n\",\n",
    "      \"Face detected with probability: 0.999991\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"aligned = []\\n\",\n",
    "    \"names = []\\n\",\n",
    "    \"for x, y in loader:\\n\",\n",
    "    \"    x_aligned, prob = mtcnn(x, return_prob=True)\\n\",\n",
    "    \"    if x_aligned is not None:\\n\",\n",
    "    \"        print('Face detected with probability: {:8f}'.format(prob))\\n\",\n",
    "    \"        aligned.append(x_aligned)\\n\",\n",
    "    \"        names.append(dataset.idx_to_class[y])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### Calculate image embeddings\\n\",\n",
    "    \"\\n\",\n",
    "    \"MTCNN will return images of faces all the same size, enabling easy batch processing with the Resnet recognition module. Here, since we only have a few images, we build a single batch and perform inference on it. \\n\",\n",
    "    \"\\n\",\n",
    "    \"For real datasets, code should be modified to control batch sizes being passed to the Resnet, particularly if being processed on a GPU. For repeated testing, it is best to separate face detection (using MTCNN) from embedding or classification (using InceptionResnetV1), as calculation of cropped faces or bounding boxes can then be performed a single time and detected faces saved for future use.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 7,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"aligned = torch.stack(aligned).to(device)\\n\",\n",
    "    \"embeddings = resnet(aligned).detach().cpu()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### Print distance matrix for classes\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 8,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"                angelina_jolie  bradley_cooper  kate_siegel  paul_rudd  \\\\\\n\",\n",
    "      \"angelina_jolie        0.000000        1.344806     0.781201   1.425579   \\n\",\n",
    "      \"bradley_cooper        1.344806        0.000000     1.256238   0.922126   \\n\",\n",
    "      \"kate_siegel           0.781201        1.256238     0.000000   1.366423   \\n\",\n",
    "      \"paul_rudd             1.425579        0.922126     1.366423   0.000000   \\n\",\n",
    "      \"shea_whigham          1.448495        0.891145     1.416447   0.985438   \\n\",\n",
    "      \"\\n\",\n",
    "      \"                shea_whigham  \\n\",\n",
    "      \"angelina_jolie      1.448495  \\n\",\n",
    "      \"bradley_cooper      0.891145  \\n\",\n",
    "      \"kate_siegel         1.416447  \\n\",\n",
    "      \"paul_rudd           0.985438  \\n\",\n",
    "      \"shea_whigham        0.000000  \\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"dists = [[(e1 - e2).norm().item() for e2 in embeddings] for e1 in embeddings]\\n\",\n",
    "    \"print(pd.DataFrame(dists, columns=names, index=names))\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.7.3\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venev",
   "language": "python",
   "name": "venev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
